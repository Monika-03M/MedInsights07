{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thxsbyCjpbbj",
        "outputId": "dcb04b07-fda8-42bc-b0e4-68869902086d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import joblib\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drug_data.csv')"
      ],
      "metadata": {
        "id": "-24I3OumqOj1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please enter your drug review: I used skyla it worked well\n",
        "\n",
        "Top words for each topic:\n",
        "Topic 1: skyla, used, worked, well\n",
        "Topic 2: well, worked, skyla, used\n",
        "Topic 3: used, skyla, worked, well\n",
        "\n",
        "Topic distribution for the review:\n",
        "Topic 1: 0.23\n",
        "Topic 2: 0.62\n",
        "Topic 3: 1.33\n",
        "\n",
        "Dominant topic: Topic 3"
      ],
      "metadata": {
        "id": "Vtbue2SSemHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Add domain-specific stopwords\n",
        "    domain_stopwords = {'mg', 'tablet', 'capsule', 'pill', 'dose', 'doctor', 'prescription'}\n",
        "    stop_words.update(domain_stopwords)\n",
        "\n",
        "    return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "def peform_topic_modeling(text):\n",
        "    # Preprocess the user's review\n",
        "    processed_review = preprocess_text(text)\n",
        "\n",
        "    # Create TF-IDF matrix\n",
        "    vectorizer = TfidfVectorizer(max_df=1.0, min_df=1)  # Adjust for single document\n",
        "    tfidf_matrix = vectorizer.fit_transform([processed_review])\n",
        "\n",
        "    # Apply NMF\n",
        "    num_topics = 3  # Using a small number of topics for a single review\n",
        "    nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "    nmf_output = nmf_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    num_top_words = 5\n",
        "\n",
        "    all_top_words = []\n",
        "    topic_distributions = []\n",
        "\n",
        "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
        "        all_top_words.append(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "    for i, topic_prob in enumerate(nmf_output[0]):\n",
        "        topic_distributions.append(f\"Topic {i+1}: {topic_prob:.2f}\")\n",
        "\n",
        "    dominant_topic = nmf_output[0].argmax() + 1\n",
        "    dominant_topic_text = f\"Dominant topic: Topic {dominant_topic}\"\n",
        "\n",
        "    return ' '.join(all_top_words), ' '.join(topic_distributions), dominant_topic_text\n",
        "\n",
        "# Get user input\n",
        "user_review = input(\"Please enter your drug review: \")\n",
        "\n",
        "a, b, c = peform_topic_modeling(user_review)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF2-UJzEhjac",
        "outputId": "8b673b42-59b3-4b39-a5fe-17d69631386a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your drug review:  I used skyla it worked well\n",
            "Topic 1: skyla, used, worked, well Topic 2: well, worked, skyla, used Topic 3: used, skyla, worked, well\n",
            "Topic 1: 0.23 Topic 2: 0.62 Topic 3: 1.33\n",
            "Dominant topic: Topic 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a95lmx5xixUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}